<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Networking | Luke Gorrie's blog]]></title>
  <link href="http://blog.lukego.com//blog/categories/networking/atom.xml" rel="self"/>
  <link href="http://blog.lukego.com//"/>
  <updated>2013-06-23T14:29:19+02:00</updated>
  <id>http://blog.lukego.com//</id>
  <author>
    <name><![CDATA[Luke Gorrie]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Recent talks]]></title>
    <link href="http://blog.lukego.com//blog/2013/04/19/recent-talks/"/>
    <updated>2013-04-19T09:42:00+02:00</updated>
    <id>http://blog.lukego.com//blog/2013/04/19/recent-talks</id>
    <content type="html"><![CDATA[<p>I have given a few talks recently:</p>

<p><a href="http://www.youtube.com/watch?v=eZDWJfB9XY4&amp;list=PL4th0AZixyRE9bb8OevAb7I8RuaWJQWnO">SLIME at the Emacs Conference</a> (16 min). History of the <a href="http://en.wikipedia.org/wiki/SLIME">SLIME</a> project and tips for people writing new Emacs-based IDEs.</p>

<p><a href="http://cast.switch.ch/vod/clips/relq1zlrq/">Snabb Switch at the Swiss OpenStack User Group</a> (8 min). Introducing the <a href="http://snabb.co/snabbswitch/">Snabb Switch</a> project to people in the local OpenStack cloud computing community.</p>

<p><a href="http://cast.switch.ch/vod/clips/26uo9i576i/">Snabb Switch at the EduPERT workshop</a> (27 min). What networking problems can you address with x86 servers? and more on Snabb Switch.</p>

<p><a href="http://blip.tv/eclm/eclm2011-luke-gorrie-6263137">Teclo Networks at ECLM 2011</a> (40 min). The early days of <a href="http://www.teclo.net/">teclo.net</a> the telecom startup company founded by Common Lisp hackers.</p>

<p>This has been really fun!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Routing traffic between local applications on Linux]]></title>
    <link href="http://blog.lukego.com//blog/2013/04/13/routing-traffic-between-local-applications-on-linux/"/>
    <updated>2013-04-13T19:48:00+02:00</updated>
    <id>http://blog.lukego.com//blog/2013/04/13/routing-traffic-between-local-applications-on-linux</id>
    <content type="html"><![CDATA[<p>Want to create a really interesting virtual network on your own host and test it with ordinary applications? Great! Here is how.</p>

<p>We will make the address <code>192.168.100.1</code> act like <code>127.0.0.1</code> but route packets through a custom network topology before processing them.</p>

<p>First start with any custom topology. In this example: <code>west</code> and <code>east</code> are endpoints with an Open vSwitch bridge <code>ovs</code> in between. (This would be great for applying OpenFlow rules to packets sent between local applications.)</p>

<p>```
ip link add west type veth peer name west_to_ovs
ip link add east type veth peer name east_to_ovs</p>

<p>ovs-vsctl add-br ovs
ovs-vsctl add-port ovs west_to_ovs
ovs-vsctl add-port ovs east_to_ovs</p>

<p>for i in west west_to_ovs east east_to_ovs ovs; do
  ip link set $i up
done
```</p>

<p>Now assign addresses and routes for these interfaces. Packets sent to <code>192.168.100.1</code> should first be routed into interface <code>west</code> then switched via <code>ovs</code> and finally delivered to <code>east</code> for processing.</p>

<p>```
ip addr add 192.168.100.2 dev west
ip addr add 192.168.100.1 dev east</p>

<p>ip route add 192.168.100.1 dev west
ip route add 192.168.100.2 dev east
```</p>

<p>The ingredients are in place but they don't <em>work</em> yet. If you ping <code>192.168.100.1</code> then the packets are sent to <code>lo</code> instead of being routed through the bridge.</p>

<p>And that brings us to the trick: <a href="http://lartc.org/howto/lartc.rpdb.html">Policy Routing</a>.</p>

<p>First make Linux globally "forget" that these addresses are local.</p>

<p><code>
ip route del 192.168.100.2 table local
ip route del 192.168.100.1 table local
</code></p>

<p>Now packets sent to <code>192.168.100.1</code> do get routed down the right path. They are not processed at the other end though, because Linux does not remember they are local. We are half way there.</p>

<p>Next create separate routing tables strictly for when packets are received after they have traversed the switch. These tables remember that the addresses are local.</p>

<p>```
ip rule add iif west lookup 100
ip route add local 192.168.100.2 dev west table 100
echo 1 > /proc/sys/net/ipv4/conf/west/accept_local</p>

<p>ip rule add iif east lookup 101
ip route add local 192.168.100.1 dev east table 101
echo 1 > /proc/sys/net/ipv4/conf/east/accept_local
```</p>

<p>Now we are done!</p>

<p>If you connect to <code>192.168.100.1</code> then your packets will first traverse the bridge and then be processed locally. The setup is symmetric so that return traffic will be routed back through the bridge too. This will work with all your favourite programs like <code>ping</code>, <code>curl</code>, <code>apache</code>, etc. Check it out by running <code>tcpdump</code> on <code>west</code> or <code>east</code>.</p>

<p>Go ahead and create interesting virtual networks on your own machine.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intel's Performance Counter Monitor]]></title>
    <link href="http://blog.lukego.com//blog/2012/08/02/nehalem-i-slash-o-performance/"/>
    <updated>2012-08-02T17:33:00+02:00</updated>
    <id>http://blog.lukego.com//blog/2012/08/02/nehalem-i-slash-o-performance</id>
    <content type="html"><![CDATA[<p>Wanna find the limiting factor in I/O performance on an Intel Nehalem server app? Here's one idea.</p>

<p>The Nehalem architecture looks like this:<br>
<a href="http://www.qdpma.com/SystemArchitecture/SystemArchitecture_QPI.html">
<img src="/images/Xeon5500_1IOH.png" alt="Nehalem Architecture" />
</a></p>

<p>So the main potential bottlenecks are:</p>

<ol>
<li>RAM<->CPU @ approx 256Gbps to each CPU's RAM bank. (Too many memory copies?)</li>
<li>CPU<->CPU @ approx 200Gbps for sharing RAM over <a href="http://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect">QPI</a>. (NUMA all mucked up?)</li>
<li>PCIe @ approx 72Gbps. (Out of ports? Crappy motherboard?)</li>
<li>Storage disk/SSD. (Waiting all the time?)</li>
</ol>


<p>Intel's <a href="http://software.intel.com/en-us/articles/intel-performance-counter-monitor/">Performance Counter Monitor</a> can tell you the utilization of these links. It's like <code>top</code> for memory bandwidth, inter-processor data shuffling, PCIe load, etc. I used this tool for verifying that PCs can push 40Gbps of full-duplex ethernet traffic without any hassle. Intel make some really fine tech.</p>

<p>Give it a try! It's really easy to install.</p>

<p>Here's how it looks:</p>

<pre>
 EXEC  : instructions per nominal CPU cycle
 IPC   : instructions per CPU cycle
 FREQ  : relation to nominal CPU frequency='unhalted clock ticks'/'invariant timer ticks' (includes Intel Turbo Boost)
 AFREQ : relation to nominal CPU frequency while in active state (not in power-saving C state)='unhalted clock ticks'/'invariant timer ticks while in C0-state'  (includes Intel Turbo Boost)
 L3MISS: L3 cache misses 
 L2MISS: L2 cache misses (including other core's L2 cache *hits*) 
 L3HIT : L3 cache hit ratio (0.00-1.00)
 L2HIT : L2 cache hit ratio (0.00-1.00)
 L3CLK : ratio of CPU cycles lost due to L3 cache misses (0.00-1.00), in some cases could be >1.0 due to a higher memory latency
 L2CLK : ratio of CPU cycles lost due to missing L2 cache but still hitting L3 cache (0.00-1.00)
 READ  : bytes read from memory controller (in GBytes)
 WRITE : bytes written to memory controller (in GBytes)


 Core (SKT) | EXEC | IPC  | FREQ  | AFREQ | L3MISS | L2MISS | L3HIT | L2HIT | L3CLK | L2CLK  | READ  | WRITE 

   0    0     0.59   0.60   0.98    1.00    1743 K   3978 K    0.56    0.62    0.15    0.07     N/A     N/A
   1    0     0.61   0.62   0.97    1.00    2595 K   3356 K    0.23    0.64    0.23    0.02     N/A     N/A
   2    0     0.49   0.59   0.83    1.00    2205 K   3198 K    0.31    0.60    0.22    0.03     N/A     N/A
   3    0     0.06   0.32   0.18    1.00     715 K    921 K    0.22    0.35    0.34    0.02     N/A     N/A
------------------------------------------------------------------------------------------------------------
 TOTAL  *     0.43   0.59   0.74    1.00    7259 K     11 M    0.37    0.61    0.21    0.04    6.51    2.85

 Instructions retired: 3707 M ; Active cycles: 6317 M ; Time (TSC): 2134 Mticks ; C0 (active,non-halted) core residency: 73.99 %

 PHYSICAL CORE IPC                 : 0.59 => corresponds to 14.67 % utilization for cores in active state
 Instructions per nominal CPU cycle: 0.43 => corresponds to 10.86 % core utilization over time interval
----------------------------------------------------------------------------------------------
</pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Shared memory: A moment of appreciation]]></title>
    <link href="http://blog.lukego.com//blog/2012/07/08/shared-memory-a-moment-of-appreciation/"/>
    <updated>2012-07-08T03:46:00+02:00</updated>
    <id>http://blog.lukego.com//blog/2012/07/08/shared-memory-a-moment-of-appreciation</id>
    <content type="html"><![CDATA[<p>Today I'm feeling appreciative of <a href="http://en.wikipedia.org/wiki/Shared_memory">shared memory</a>.</p>

<p>There are a lot of communication mechanisms in widespread use: C APIs, system calls, RESTful services, JSON-RPC, AMQP, and plenty more besides. Some mechanisms survive, a lot influence new successors, and quite a lot of them just fall out of fashion and turn to dust. Shared memory is one with a timeless quality. Year after year it's the glue that holds our computers together.</p>

<p>Shared memory is a simple mechanism. Many different programs are concurrently accessing the physical RAM chips in the ocmputer. The programs are written in completely different programming languages, they are executing on different microchips, and some of them are implemented as hardware rather than software. The programs all share a common abstraction: LOAD and STORE operations towards a big array of machine words. The rest is a matter of conventions, design patterns, and data structures. That's all we need!</p>

<p>I really enjoy the versatility. How sweet it is to be able to drive hardware memory controller so directly.</p>

<h3>Playing around</h3>

<p>Just for fun, here's a shared memory interface I'm playing with for ethernet networking. That is: to use shared memory to send and receive packets between a "host" and a "device". The host and device can be written in any programming language, or indeed in hardware, and this is roughly how hardware packet I/O interfaces really look.</p>

<p>Here's an example memory layout. I sketched it using C syntax and defined a packet <a href="http://en.wikipedia.org/wiki/Circular_buffer">ring buffer</a> for transmision in each direction.</p>

<p>```c
enum {</p>

<pre><code>RING_SIZE = 1024,
PACKET_SIZE = 1504,
</code></pre>

<p>};</p>

<p>// One ethernet packet.
struct packet
{</p>

<pre><code>unsigned int length;         // length of packet
char data[PACKET_SIZE];      // on-the-wire packet data
</code></pre>

<p>};</p>

<p>// Ring buffer containing ethernet packets.
struct ring
{</p>

<pre><code>unsigned int head;           // position of first unread packet
unsigned int tail;           // position of last unread packet
unsigned int overflow_count; // number of packets dropped due to overflow
struct packet packets[RING_SIZE];
</code></pre>

<p>};</p>

<p>// Ethernet device with two ring buffers for transmit and receive.
struct dev
{</p>

<pre><code>unsigned int magic;          // magic number identifying the structure
unsigned int version;        // version of memory layout
struct ring dev2host;        // ring of packets from host to device
struct ring host2dev;        // ring of packets from device to host
</code></pre>

<p>};
```</p>

<p>Here's a snippet using the data structure from C:</p>

<p>```c
bool is_empty(struct ring ring) {</p>

<pre><code>return ring-&gt;head == ring-&gt;tail;
</code></pre>

<p>}
```</p>

<p>and here's a snippet from LuaJIT:</p>

<p><code>lua
function is_full (ring)
   return ring.head == (ring.tail + 1) % C.RING_SIZE
end
</code></p>

<p>and with any luck I'll find an excuse to see how it looks in a hardware description language like Verilog too.</p>

<p>That's all. I really appreciate shared memory. Hardware people use it all the time. It's not the most high level communication protocol around, but it is simple and accessible, and it can be fun for software people too. I think so, anyway :-)</p>
]]></content>
  </entry>
  
</feed>
