<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Networking | Luke Gorrie's weblog]]></title>
  <link href="http://blog.lukego.com//blog/categories/networking/atom.xml" rel="self"/>
  <link href="http://blog.lukego.com//"/>
  <updated>2012-09-25T11:43:10+02:00</updated>
  <id>http://blog.lukego.com//</id>
  <author>
    <name><![CDATA[Luke Gorrie]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Intel's Performance Counter Monitor]]></title>
    <link href="http://blog.lukego.com//blog/2012/08/02/nehalem-i-slash-o-performance/"/>
    <updated>2012-08-02T17:33:00+02:00</updated>
    <id>http://blog.lukego.com//blog/2012/08/02/nehalem-i-slash-o-performance</id>
    <content type="html"><![CDATA[<p>Wanna find the limiting factor in I/O performance on an Intel Nehalem server app? Here's one idea.</p>

<p>The Nehalem architecture looks like this:<br>
<a href="http://www.qdpma.com/SystemArchitecture/SystemArchitecture_QPI.html">
<img src="/images/Xeon5500_1IOH.png" alt="Nehalem Architecture" />
</a></p>

<p>So the main potential bottlenecks are:</p>

<ol>
<li>RAM<->CPU @ approx 256Gbps to each CPU's RAM bank. (Too many memory copies?)</li>
<li>CPU<->CPU @ approx 200Gbps for sharing RAM over <a href="http://en.wikipedia.org/wiki/Intel_QuickPath_Interconnect">QPI</a>. (NUMA all mucked up?)</li>
<li>PCIe @ approx 72Gbps. (Out of ports? Crappy motherboard?)</li>
<li>Storage disk/SSD. (Waiting all the time?)</li>
</ol>


<p>Intel's <a href="http://software.intel.com/en-us/articles/intel-performance-counter-monitor/">Performance Counter Monitor</a> can tell you the utilization of these links. It's like <code>top</code> for memory bandwidth, inter-processor data shuffling, PCIe load, etc. I used this tool for verifying that PCs can push 40Gbps of full-duplex ethernet traffic without any hassle. Intel make some really fine tech.</p>

<p>Give it a try! It's really easy to install.</p>

<p>Here's how it looks:</p>

<pre>
 EXEC  : instructions per nominal CPU cycle
 IPC   : instructions per CPU cycle
 FREQ  : relation to nominal CPU frequency='unhalted clock ticks'/'invariant timer ticks' (includes Intel Turbo Boost)
 AFREQ : relation to nominal CPU frequency while in active state (not in power-saving C state)='unhalted clock ticks'/'invariant timer ticks while in C0-state'  (includes Intel Turbo Boost)
 L3MISS: L3 cache misses 
 L2MISS: L2 cache misses (including other core's L2 cache *hits*) 
 L3HIT : L3 cache hit ratio (0.00-1.00)
 L2HIT : L2 cache hit ratio (0.00-1.00)
 L3CLK : ratio of CPU cycles lost due to L3 cache misses (0.00-1.00), in some cases could be >1.0 due to a higher memory latency
 L2CLK : ratio of CPU cycles lost due to missing L2 cache but still hitting L3 cache (0.00-1.00)
 READ  : bytes read from memory controller (in GBytes)
 WRITE : bytes written to memory controller (in GBytes)


 Core (SKT) | EXEC | IPC  | FREQ  | AFREQ | L3MISS | L2MISS | L3HIT | L2HIT | L3CLK | L2CLK  | READ  | WRITE 

   0    0     0.59   0.60   0.98    1.00    1743 K   3978 K    0.56    0.62    0.15    0.07     N/A     N/A
   1    0     0.61   0.62   0.97    1.00    2595 K   3356 K    0.23    0.64    0.23    0.02     N/A     N/A
   2    0     0.49   0.59   0.83    1.00    2205 K   3198 K    0.31    0.60    0.22    0.03     N/A     N/A
   3    0     0.06   0.32   0.18    1.00     715 K    921 K    0.22    0.35    0.34    0.02     N/A     N/A
------------------------------------------------------------------------------------------------------------
 TOTAL  *     0.43   0.59   0.74    1.00    7259 K     11 M    0.37    0.61    0.21    0.04    6.51    2.85

 Instructions retired: 3707 M ; Active cycles: 6317 M ; Time (TSC): 2134 Mticks ; C0 (active,non-halted) core residency: 73.99 %

 PHYSICAL CORE IPC                 : 0.59 => corresponds to 14.67 % utilization for cores in active state
 Instructions per nominal CPU cycle: 0.43 => corresponds to 10.86 % core utilization over time interval
----------------------------------------------------------------------------------------------
</pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Shared memory: A moment of appreciation]]></title>
    <link href="http://blog.lukego.com//blog/2012/07/08/shared-memory-a-moment-of-appreciation/"/>
    <updated>2012-07-08T03:46:00+02:00</updated>
    <id>http://blog.lukego.com//blog/2012/07/08/shared-memory-a-moment-of-appreciation</id>
    <content type="html"><![CDATA[<p>Today I'm feeling appreciative of <a href="http://en.wikipedia.org/wiki/Shared_memory">shared memory</a>.</p>

<p>There are a lot of communication mechanisms in widespread use: C APIs, system calls, RESTful services, JSON-RPC, AMQP, and plenty more besides. Some mechanisms survive, a lot influence new successors, and quite a lot of them just fall out of fashion and turn to dust. Shared memory is one with a timeless quality. Year after year it's the glue that holds our computers together.</p>

<p>Shared memory is a simple mechanism. Many different programs are concurrently accessing the physical RAM chips in the ocmputer. The programs are written in completely different programming languages, they are executing on different microchips, and some of them are implemented as hardware rather than software. The programs all share a common abstraction: LOAD and STORE operations towards a big array of machine words. The rest is a matter of conventions, design patterns, and data structures. That's all we need!</p>

<p>I really enjoy the versatility. How sweet it is to be able to drive hardware memory controller so directly.</p>

<h3>Playing around</h3>

<p>Just for fun, here's a shared memory interface I'm playing with for ethernet networking. That is: to use shared memory to send and receive packets between a "host" and a "device". The host and device can be written in any programming language, or indeed in hardware, and this is roughly how hardware packet I/O interfaces really look.</p>

<p>Here's an example memory layout. I sketched it using C syntax and defined a packet <a href="http://en.wikipedia.org/wiki/Circular_buffer">ring buffer</a> for transmision in each direction.</p>

<p>```c
enum {</p>

<pre><code>RING_SIZE = 1024,
PACKET_SIZE = 1504,
</code></pre>

<p>};</p>

<p>// One ethernet packet.
struct packet
{</p>

<pre><code>unsigned int length;         // length of packet
char data[PACKET_SIZE];      // on-the-wire packet data
</code></pre>

<p>};</p>

<p>// Ring buffer containing ethernet packets.
struct ring
{</p>

<pre><code>unsigned int head;           // position of first unread packet
unsigned int tail;           // position of last unread packet
unsigned int overflow_count; // number of packets dropped due to overflow
struct packet packets[RING_SIZE];
</code></pre>

<p>};</p>

<p>// Ethernet device with two ring buffers for transmit and receive.
struct dev
{</p>

<pre><code>unsigned int magic;          // magic number identifying the structure
unsigned int version;        // version of memory layout
struct ring dev2host;        // ring of packets from host to device
struct ring host2dev;        // ring of packets from device to host
</code></pre>

<p>};
```</p>

<p>Here's a snippet using the data structure from C:</p>

<p>```c
bool is_empty(struct ring ring) {</p>

<pre><code>return ring-&gt;head == ring-&gt;tail;
</code></pre>

<p>}
```</p>

<p>and here's a snippet from LuaJIT:</p>

<p><code>lua
function is_full (ring)
   return ring.head == (ring.tail + 1) % C.RING_SIZE
end
</code></p>

<p>and with any luck I'll find an excuse to see how it looks in a hardware description language like Verilog too.</p>

<p>That's all. I really appreciate shared memory. Hardware people use it all the time. It's not the most high level communication protocol around, but it is simple and accessible, and it can be fun for software people too. I think so, anyway :-)</p>
]]></content>
  </entry>
  
</feed>
